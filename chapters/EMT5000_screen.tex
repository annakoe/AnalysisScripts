\begin{footnotesize}

\section{Bioinformatic analysis of screens with EMT5000 control library and stable cell lines}
\label{sec:Bioinformatic analysis of screens with EMT5000 control library and stable cell lines}

\subsection{Read trimming and QC}

sequenced on the HiSeq. Reads have the following general structure:
\begin{lstlisting}
NNNNNNNAAGTATTTCGATTTCTTGGCTTTATATATCTTGTGGAAAGGACGAAACACCG-N19-GTTTTAGAGCTAGAAATAGCAAGTTAAAATAAGGCTAGTCCGNNNNNNN
\end{lstlisting}

whereby the first 7 N bases are the 5' barcode, followed by the plasmid 'stuffer',  GN19 denotes the gRNA sequence from the EMT5000 library, which is followed by another plasmid sequence and the last 7 N bases are the 3' barcode. The 5' and 3' barcodes serve as unique molecular identifiers (UMIs), allowing counting of original gRNA sequences extracted from lentivirus-infected cells by removing PCR-amplification bias.

Sequences from Lane1 and Lane2 of the flow cell were combined using the unix 'cat' command. Next, the 5' barcode was extracted from the reads using cutadapt (version 1.2.1), requiring a minimum overlap of 35 bp between the plasmid stuffer sequence and the read with a maximum error of 10 \% and a minimum barcode length of 7 bp.

\begin{lstlisting}
Command line parameters:
-a AAGTATTTCGATTTCTTGGCTTTATATATCTTGTGGAAAGGACGAAACACC -O 35 -m 7 
\end{lstlisting}

The 3' barcode was retrieved from the read in an analogous way:

\begin{lstlisting}
Command line parameters:
-g GTTTTAGAGCTAGAAATAGCAAGTTAAAATAAGGCTAGTCCG -O 28 -m 7
\end{lstlisting}

Finally the gRNA sequence was extracted from the read, requiring a minimum length of 2 bp (to discard reads that contain no gRNAs and are derived from primer dimers):

\begin{lstlisting}
Command line parameters:
cutadapt -g AAGTATTTCGATTTCTTGGCTTTATATATCTTGTGGAAAGGACGAAACACC -a GTTTTAGAGCTAGAAATAGCAAGTTAAAATAAGGCTAGTCCG -n 2 -m 2 -O 10
\end{lstlisting}

Next, the barcode and gRNA reads were quality-filtered using $fastq_quality_filter$ from the fastx-toolbox (version XXXXX). Reads where any base has a Quality score of less than 20 were discarded. 

\begin{lstlisting}
Command line parameters: -Q33 -q 20 -p 1 
\end{lstlisting}

Subsequently files were converted from fastq to fasta format using $fastq_to_fasta$. 

\begin{lstlisting}
fastq_to_fasta -Q33 -n -i  inputfile.fastq -o outputlfile.fasta
\end{lstlisting}

gRNA reads were subsequently aligned back onto the indexed EMT5000 reference library using bwa (version: 0.6.2-r126), allowing 2 mismatches and default open gaps (1 indel).

\begin{lstlisting}
Command line options:
bwa aln -n 2 -l 5 -N -I  
bwa samse -n 10000 
\end{lstlisting}


The indexed EMT5000 library file used as a reference for alignment was derived from the file $GN20GG_masked_autoXY_EMT_genepromoter_comprehensive_complete_noPAM_unique_strand_PAMremoved.fa $ (see section \ref{Bioinf_methods: EMT5000 library}) using bwa index:

\begin{lstlisting}
bwa index GN20GG_masked_autoXY_EMT_genepromoter_comprehensive_complete_noPAM_unique_strand_PAMremoved.fa
\end{lstlisting}


Following alignment using bwa, reads that mapped uniquely to the forward strand were extracted using samtools (version XXXX) as follows:

\begin{lstlisting}
samtools view -F 20 -q 1 -S aligned_gRNA.sam  > gRNA_uniquely_mapped.sam
\end{lstlisting}


To ask how many different gRNAs from the library were sequenced in each sample

\begin{lstlisting}
samtools view -F 20 -S  aligned_gRNA.sam | cut -f 3 | sort | uniq | wc -l
\end{lstlisting}

For subsequent analysis it was necessary to construct a tab-separated file of the following format:
want to get a tab-separated 3 column file containing for each read the  FASTA identifier (read-ID), gRNA chr:start-end and barcode.

to get the gRNA:
\begin{lstlisting}
samtools view -F 20 -q 1 -S aligned_gRNA.sam | awk '{print$1 ".\t" $3}'  > gRNA_uniquelymapped
\end{lstlisting}

to get the FASTA identifier:
\begin{lstlisting}
samtools view -F 20 -q 1 -S aligned_gRNA.sam | awk '{print$1 "."}' > gRNA_uniquelymapped_readID
\end{lstlisting}

The files containing the quality-filtered 5' and 3' barcode (UMI sequence) generated above, were modified as follows (pseudocode shown for 5' bc only):

\begin{lstlisting}
for i in *_5bc_Q20.fasta
do cat "$i" | paste - - | awk -F ' ' '{print $1 ".\t" $3}' > "${i%_5bc_Q20.fasta}"_5bc_Q20_point.fasta;
done
\end{lstlisting}

Next, only the barcodes associated with gRNAs that aligned uniquely were retrieved from the barcode file using grep:

\begin{lstlisting}
for i in *_5bc_Q20_point.fasta
do grep -wFf "${i%_5bc_Q20_point.fasta}"gRNA_uniquelymapped_readID "$i" > "${i%_5bc_Q20_point.fasta}"gRNA_uniquelymapped_readID_with_5bc;
done
\end{lstlisting}

For each read, identified by its readID, the gRNA sequence and 5' and 3' barcodes were combined into a single file. The three files were joined (finding the union) using the JOIN command, which requires the files to be sorted in the following way:

\begin{lstlisting}
for i in *gRNA_uniquelymapped;
do awk -F "\t" '{print ">" $1 "\t" $2}' "$i"| sort -k 1b,1 > "${i%}"_sorted;
done

for i in *gRNA_uniquelymapped_readID_with_3bc;
do sort -k 1b,1 "$i" > "${i%}"_sorted;
done

for i in *gRNA_uniquelymapped_readID_with_5bc; do sort -k 1b,1 "$i" > "${i%}"_sorted;
done
\end{lstlisting}

Next, the three files were joined as follows:

\begin{lstlisting}
for i in *gRNA_uniquelymapped_sorted;
do join "$i" "${i%gRNA_uniquelymapped_sorted}"gRNA_uniquelymapped_readID_with_5bc_sorted | join - "${i%gRNA_uniquelymapped_sorted}"gRNA_uniquelymapped_readID_with_3bc_sorted > "${i%gRNA_uniquelymapped_sorted}"gRNA_uniquelymapped_readID_gRNA_5bc_3bc_length14;
done
\end{lstlisting}

This yields a file containing for each uniquely mapped read its readID, the gRNA it mapped to (chr:start-stop) and the UMI found in the read (of length exactly 14 bp).



\subsection{Deriving gRNA counts from UMI-barcodes without PCR error correction}

The gRNA counts were derived by counting the number of times each gRNA occurs together with a different UMI in the sequencing data.

The script collapse-barcodes.py was run using a maximum edit distance of 0, i.e. not correcting any PCR errors that might have occurred in the barcode.

\begin{lstlisting}
python collapse_barcodes.py Samplefilename 0
\end{lstlisting}


\begin{lstlisting}
# collapse_barcodes.py v 1.1
# Anna Koeferle Dec 2015, UCL
# adapted from CollapseTCRs.py v1.2 by James Heather and Katharine Best

### BACKGROUND ###
# Makes use of random molecular barcode sequences to error-correct high-throughput sequencing data.
# The barcodes are random nucleotides (N) added to the library amplicon prior to amplification.
# Instead of counting reads/molecules (which is problematic due to PCR amplification bias), the co-occurence of different barcodes with a sequence of interst is counted (count barcodes instead of reads).
# This allows us to correct for PCR amplification bias and PCR error/sequencing error.
# The script does the following:
# 1. Import input file of the form (c1) identifier (c2) gRNA chr:start-end OR DNA sequence (c3) barcode (varying lengths)
# 2. The script then sorts according to (c2) gRNA.
# 3. Within each gRNA, the barcodes are grouped according to sequence similarity
# If a barcode is within x edit distance from the previous one group them together
# 4. barcodes in groups are re-written/corrected to the sequence of the most common member of the group
# 5. export a file that has the corrected barcode sequence in c4

### INPUT ###
# takes a tab-delimited file consisting of 3 columns:
# column1: Read identifier
# column2: gRNA from the library read was aligned back to, either as DNA sequence or coordinates of the form chr:start-end
# column3: barcode (various lengths N)
# Run: python CollapseBarcodess.py FILENAME.n12

### OUTPUT ###
### Outputs a gRNA frequency file = gRNA and its counts
## can output a .dict file = the collapsed dictionaries used to calculate the gRNA frequencies, by default commented out

### USAGE ###
# python CollapseBarcode.py INPUTFILENAME MAX_NUMBER_OF_ERRORS

#### PACKAGES ####

from __future__ import division
import collections as coll
import sys
import Levenshtein as lev
import re
from operator import itemgetter
from time import time, clock
import json
import signal
from Bio.Seq import Seq
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from StringIO import StringIO
import numpy as np
import matplotlib.pyplot as plt
import pylab as pylab

##### Setting functions and empty dictionaries for later use
def dodex():
    return coll.Counter()

def breakdown(etc):
  # Used to break a given dcr_etc (i.e. what is stored in a given line of the inputfile) into its components
  # Splits on '|', to avoid breaking up within identifiers or fastq quality strings

  return re.findall(r"[\w,\<\=\>\-\;\:\?\/\.\@\# ]+", str(etc))
  # breakdown[0] = gRNA / [1] = ID

def guide_frequency(collections_dictionary):
    output=[]
    for i in collections_dictionary:
        clustered = coll.Counter()
        for k,v in collections_dictionary[i].most_common():
            clustered[k] += v
        result = str(i) + ", " + str(len(clustered)) + "\n" # len of dictionary returns number of key:values stores output in string
        output.append(result)

    return output

def remove_orphan_barcodes(diction):
    #removes barcodes that are seen only once, even after error correction
    dict_morethan1 = coll.defaultdict(list)

    for x in diction: #loop over the gRNAs
        counter = coll.Counter() # set an empty counter
        for k,v in diction[x].most_common():  # loop over the barcodes and counts for each gRNA
            if v > 1:    # if the count is greater than 1
                counter[k] += v   # add the barcode and counter to the counter dictionary
        dict_morethan1[x] = counter   # append to the gRNAs

    return dict_morethan1


#### Importing data from input###

total_number_of_reads = 0 # count_input_lines

if (len(sys.argv) <> 2):
    print "Missing inputfile! Usage: python collapse_barcodes_editdist0.py INPUTFILENAME"
    sys.exit()
else:
    seqfilename = str(sys.argv[1])    # stores name of user-supplied file in variable seqfilename
    distance_threshold = int(sys.argv[2]) #stores user-supplied max number of edit distances for barcodes to be grouped together
    start_time = time() # set time at start

    print "\nReading", str(seqfilename), "into dictionary..."
    t0 = time() # Begin timer

### TAB- DELIMITED POSITIONS IN THE INPUTFILE ARE: ###

# Read identifier -[0]-
# gRNA -[1]-
# Barcode-[2]-

    dict_seqfile = coll.defaultdict(list)   #make an empty collections default dictionary

    with open(seqfilename) as infile:
        for line in infile:
            total_number_of_reads += 1
            lsplit = line.strip().split('\t')
            readID = lsplit[0]
            gRNA = lsplit[1]
            barcode = lsplit[2] #this includes barcodes of varying lengths

            values = gRNA + "|" + readID
            dict_seqfile[barcode].append(values) # this now is a dictionary with key=barcode and value=a list of gRNA and readIDs separated by "|" all with the same key, e.g. looks like 'TGTGGGCGACGGGG': ['chr10:31609045-31609068|>D00623:46:H7JVFBCXX:1:1101:5035:69776.', 'chr10:31609045-31609068|>D00623:46:H7JVFBCXX:1:1102:10781:28677.', 'chr10:31609045-31609068|>D00623:46:H7JVFBCXX:1:1102:9779:13200.', 'chr10:31609045-31609068|>D00623:46:H7JVFBCXX:1:1103:3007:69834.', 'chr10:31609045-31609068|>D00623:46:H7JVFBCXX:1:1104:1364:82596.', 'chr10:31609045-31609068|>D00623:46:H7JVFBCXX:1:1104:15350:25118.']

    timed = time() - t0
    print 'The total number of reads for this sample was:' +str(total_number_of_reads) +'.'

#### now take the inputfile and derive something that looks like dcr_collapsed dictionary of Jamie Heather:

print "Counting barcodes associated with each gRNA..."

dict_collapsed = coll.defaultdict(dodex)
t0 = time() # Begin timer


for key in dict_seqfile:  #loop throught the barcodes, i.e. the key. like saying 'for every barcode in the dictionary':
    if len(key) >= 14:   # exclude barcodes that are less than 14 bases in length
        for value in dict_seqfile[key]:          # loop through the values (gRNA | identifier)
            gRNA_seq = breakdown(value)[0] # prints only the value correspondign to gRNA
            dict_collapsed[gRNA_seq][key] += 1 # appends to dict_collapsed the gRNA sequence, the barcode = key and its count. In fact, does the counting

timed = time() - t0

#print'InputData collapsed into dictionary: '
#print dict_collapsed

print '\t Finished! Took', round(timed,3), 'seconds'

###### dict_collapsed looks like this with test datasset: {gRNA: Counter({'barcode key':count, 'barcode':count}),
#defaultdict(<function dodex at 0x103000de8>, {'chr10:31609167-31609190': Counter({'TTA': 1888, 'TTATTT': 985, 'AAGTAA': 752, 'CCCCCG': 8, 'CTCTTC': 4, 'CTTATG': 4,

print "Removing orphan barcodes..."

t0 = time() #begin timer

try:
    dict_no_orphans = remove_orphan_barcodes(dict_collapsed)
except Exception, msg:
    print str(msg)


timed = time() - t0
print '\t Finished! Took', round(timed,3), 'seconds'

###### calculate the frequencies of gRNAs based on length of the dictionaries
print "Computing gRNA frequencies..."

try:
    frequency_clustered = guide_frequency(dict_collapsed)
except Exception, msg:
    print str(msg)

with open(seqfilename.split(".")[0]+ "_frequency_raw", 'w') as outfile:
    for item in frequency_clustered:
        outfile.write(item)

########### gRNA counts for dict_threshold

try:
    frequency_no_orphans = guide_frequency(dict_no_orphans)  # run the function guide_frequency on the dictionaries generated above and store output in a string
except Exception, msg:
    print str(msg)


with open(seqfilename.split(".")[0] + "_frequency_no_orphans", 'w') as outfile: #open a csv file to write to
    for item in frequency_no_orphans: #loop through the elements of the list, and print elements of list to file
        outfile.write(item)

print "\tDONE!"

#print "Outputting dictionaries to file...."

#json.dump(dict_clustered, open(seqfilename.split(".")[0]+str("_collapsed_dict_readcountnorm"), 'w'))
#json.dump(dict_no_orphans, open(seqfilename.split(".")[0]+str("_collapsed_no_orphans_dict_readcountnorm"), 'w'))
#json.dump(dict_threshold, open(seqfilename.split(".")[0]+str("_collapsed_threshold_dict_readcountnorm"), 'w'))

\end{lstlisting}

\subsection{Assessing PCR error by plotting the number of reads per gRNA against number of different gRNA sequences}

To assess whether PCR error drives barcode diversity, we treated the gRNA part of the read like a barcode and plotted the correlation between number of reads and counts (see \ref{subsec:Diagnostic_plot Counts vs reads}). This assumes that the likelihood of introducing an error into the sequence is the same for the UMI barcodes and gRNA portions of the amplicon.

The barcode sequence was replaced with the gRNA sequence for each read to generate a tab-separated file with columns [0] readID [1] gRNA chr:start-stop [2] 'pseudo-barcode'(gRNA sequence) as follows:

\begin{lstlisting}
#get gRNA sequence 
cat Sample_gRNA_Q20.fasta | paste - - | awk -F ' ' '{print $1 ".\t" $3}' | sort > Sample_gRNA_Q20_point

#get the read ID
awk -F '\t' '{print $1}' Sample_gRNA_5bc_3bc_length14 | sort > Sample_gRNA_5bc_3bc_length14_read_ID

#get gRNA sequence for each readID
grep -wFf Sample_gRNA_5bc_3bc_length14_read_ID Sample_gRNA_Q20_point > Sample_gRNA_Q20_point_uniquely_mapped

#sort the previously generate file containing [0] readID, [1] gRNA chr:start-stop, [2] UMI of 5' and 3' barcode
sort -k 1b,1 Sample_gRNA_uniquelymapped_readID_gRNA_5bc_3bc_length14 > Sample_gRNA_uniquelymapped_readID_gRNA_5bc_3bc_length14_sorted

#sort the gRNA sequence file
sort -k 1b,1 Sample_gRNA_Q20_point_uniquely_mapped > Sample_gRNA_Q20_point_uniquely_mapped_sorted

#join the two files on readID
join Sample_gRNA_Q20_aligned_2mismatches1gap_uniquelymapped_readID_gRNA_5bc_3bc_length14_sorted Sample_gRNA_Q20_point_uniquely_mapped_sorted | awk -F ' ' '{print $1 "\t" $2 "\t" $4}' > Sample_gRNA_Q20_aligned_2mismatches1gap_uniquelymapped_readID_gRNA_instead_of_barcode
\end{lstlisting}


\subsection{Deriving gRNA counts from UMI-barcodes with naive PCR error correction}

The script collapse-barcodes.py was run using a maximum edit distance of 4. 

\begin{lstlisting}
python collapse_barcodes.py Samplefilename 4
\end{lstlisting}

This means that before counting how many different barcodes are associated with each gRNA, the barcodes are collapsed into groups. Barcodes are first ranked in decreasing order based on the number of reads harbouring its sequence. The barcode with the most reads forms the first group. If the second-ranked barcode is within 4 edit-distances of this barcode it will be assumed to have originated by PCR error and will be added to the group. If the barcode differs from the group by greater then 4 edits, it will form its own group and so on.
The number of groups per gRNA is the count (number of original gRNA-barcode combinations) after error correction.


\subsection{Bayesian PCR error correction of barcoded sequencing data }

A Bayesian error correction script was written by James E. Barrett to infer gRNA counts from the UMI data. The model takes into account the fact that the 14 bp UMI consists of a 5' and 3' barcode that was attached to the gRNA amplicon during 2 intial cycles of PCR during the sequencing library prep. The model infers the most likely number of initial gRNA-barcode data given the barcode sequences observed in the sequencing sample.

This Bayesian model takes as input the number of reads associated with each gRNA-UMI combination (without PCR error correction). I calculated these using the script make-csv-4Bayes.py. 


\begin{lstlisting}
#make_csv_4Bayes.py v 1.0
#written by Anna Koeferle, 2015
#adapted in large parts from  the script CollapseTCRs.py by Jamie Heather (can be found at https://github.com/JamieHeather/tcr-analysis/blob/master/CollapseTCRs.py)

#This script takes in a tab-separated file containing  [0] read ID [1] gRNA chr:start-end [2] barcode 5primer and 3 prime fused as input
#This script ouptuts a csv file to input into the bayesian PCR error correction script written by James E. Barrett
#The output is a csv file with columns:  [0] gRNA chr:start-end,  [1] barcode (14 bp 5' and 3' barcode combined, [2] read count for this barcode gRNA combination (no error correction)

from __future__ import division
import collections as coll
import sys
import Levenshtein as lev
import re
from operator import itemgetter
from time import time, clock
import json
import signal
from Bio.Seq import Seq
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from StringIO import StringIO
import numpy as np
import matplotlib.pyplot as plt
import pylab as pylab


def dodex():
    return coll.Counter()

def breakdown(etc):
  # Splits on '|', to avoid breaking up within identifiers or fastq quality strings

  return re.findall(r"[\w,\<\=\>\-\;\:\?\/\.\@\# ]+", str(etc))
  # breakdown[0] = gRNA / [1] = ID

def read_data(seqfilename):
    dict_seqfile = coll.defaultdict(list)   #make an empty collections default dictionary

    with open(seqfilename) as infile:
        for line in infile:
            lsplit = line.strip().split('\t')
            readID = lsplit[0]
            gRNA = lsplit[1]
            barcode = lsplit[2]
            values = gRNA + "|" + readID
            dict_seqfile[barcode].append(values)

    return dict_seqfile

def count_barcodes(dict_seqfile):

    dict_collapsed = coll.defaultdict(dodex)

    for key in dict_seqfile:  #loop throught the barcodes, i.e. the key. like saying 'for every barcode in the dictionary':
        if len(key) >= 14:   # exclude barcodes that are less than 14 bases in length
            for value in dict_seqfile[key]:          # loop through the values (gRNA | identifier)
                gRNA_seq = breakdown(value)[0] # prints only the value correspondign to gRNA
                dict_collapsed[gRNA_seq][key] += 1 # appends to dict_collapsed the gRNA sequence, the barcode = key and its count. In fact, does the counting

    return dict_collapsed

#write the dictionary into a csv file

def write_dict_to_csv(dict_clustered_name, outfilename):
    outfile = open( outfilename.split(".")[0]+str("_no_error_correction.csv"), 'w' )

    for key, value in dict_clustered_name.items():  # iterate through the coll.dictionary
        for item in value.iteritems(): #iterate through the counter
            outfile.write( str(key) + "," + ','.join(map(str, item)) + '\n')

if (len(sys.argv) <> 2):
    print "Missing inputfile! Usage: python make_csv_james_model.py INPUTFILENAME"
    sys.exit()
else:
    seqfilename = str(sys.argv[1])    # stores name of user-supplied file in variable seqfilename
    start_time = time() # set time at start

    print "\nReading", str(seqfilename), "into dictionary..."


dict_seqfile = read_data(seqfilename)
dict_collapsed = count_barcodes(dict_seqfile)

print '\t Finished! Writing to csv'

write_dict_to_csv(dict_collapsed, seqfilename)

print '\t DONE!'
\end{lstlisting}

The script was run over all samples as follows:

\begin{lstlisting}
for i in *length14; do python ../make_csv_4Bayes.py "$i"; done
\end{lstlisting}

The output of this script was then fed into a Bayesian error correction script.

\subsubsection{Bayesian PCR error correction of barcoded sequencing count data script by James E. Barrett}

This script was kindly contributed by James E. Barrett. 

The Bayesian model infers the number of unique original barcoded gRNA molecules from noise-corrupted count data. The model estimates a corrected read count, which may be interpreted as a proxy for the original noise-free number of unique barcodes associated with a particular gRNA. 

\subsubsection{Model definition}
For each gRNA we observe $N$ barcode pairs denoted by $(\vecy_{i}^1,\vecy_{i}^2)$ where the superscript denotes the first and second barcodes and $i=1,\ldots,N$. Elements of the $d$-dimensional vector $\vecy_{i}^{\eta}\in\{\verb+T+,\verb+C+,\verb+G+,\verb+A+\}^d$ where $\eta=[1,2]$. The number of corresponding sequencing reads is denoted by $\sigma_{i}\in\mathbb{Z}_+$. 

The model assumes that there exist $Q$ \emph{latent barcodes} $\vecx_1^{\eta},\ldots,\vecx_Q^{\eta}$ from which the observed barcodes are generated in a noise corrupting stochastic process (PCR amplification errors and random barcode switching). The model further assumes that for each pair $(\vecy_i^1,\vecy_i^2)$ only one of the observed barcodes is written in terms of the latent barcode via
\begin{equation}
\vecy_i^{\eta} = \sum_{q=1}^Q w_{iq}^{\eta}\theta(\vecx_{q}^{\eta})\quad\text{subject to}\quad w_{iq}^{\eta}\in[0,1]\quad\text{and}\quad\sum_{q,\eta} w_{iq}^{\eta} = 1.
\end{equation}
There is therefore only one non-zero value of $[\vecw_i^1,\vecw_i^2]$ that indicates which latent barcode the observed pair is associated with. The function $\theta$ represents a noise corrupting stochastic process where the status of each nucleotide site may be changed randomly with probability $\beta\in[0,1/2]$. We can therefore write
\begin{equation}
p(y_{i\mu}^{\eta}|x_{q\mu}^{\eta},\beta) = \left\{
\begin{array}{lr}
(1-\beta)\delta_{y_{i\mu}^{\eta}x_{q\mu}^{\eta}} + \beta (1-\delta_{y_{i\mu}^{\eta}x_{q\mu}^{\eta}})&\quad\text{if $w_{iq}^{\eta}=1$}\\
0&\quad\text{otherwise}\\
\end{array}
\right.
\end{equation}
for $\mu=1,\ldots,d$. We denote the collections of $\vecx_{q}^{\eta}$, $\vecy_i^{\eta}$ and $\vecw_i^{\eta}$ by $\matX$, $\matY$, and $\matW$ respectively. The posterior is
\begin{equation}
p(\matX,\matW|\matY,\sv,\beta) \propto p(\matY|\matX,\matW,\sv,\beta)p(\matX)p(\matW)
\end{equation}
with
\begin{align}
p(\matY|\matX,\matW,\sv,\beta) &=\prod_{i}\left[\sum_{q,\eta}w_{iq}^{\eta} p(\vecy_i^{\eta}|\vecx_q^{\eta},\beta)\right]^{\sigma_i}.
\label{eq:likelihood}
\end{align}
Maximum entropy priors for $\matX$ and $\matW$ are uniform distributions so $p(\matX)$ and $p(\matW)$ are constant.

\subsubsection{Inference of model parameters}

The Maximum A Posteriori (MAP) solution of $\matW$ is denoted by $\matW^*$. Since only one element of $[\vecw_i^1,\vecw^2_i]$ is non-zero the expression (\ref{eq:likelihood}) is maximised by selecting $\text{argmax}_{q,\eta} p(\vecy_i^{\eta}|\vecx_q^{\eta},\beta)$ as the non-zero element.

To find the MAP solution for nucleotide $\mu$ of the latent barcode indexed by $(q,\eta)$ we consider all observed barcodes that generated from it (as defined by $\matW$). If we let $n_1$ and $n_0$ denote the total number of matches and mismatches respectively between that latent barcode and the associated observed barcodes, then the corresponding data likelihood is $(1-\beta)^{n^1_{q\mu}}\beta^{n^0_{q\mu}}$. This will be maximised if the number of matches is maximised. This is achieved selecting the most common observed nucleotide as the value for the latent nucleotide (while taking into account multiple counts).

If we let $N_1$ and $N_0$ denote the total number of matches and mismatches respectively across all of the latent barcodes and observed data then we can write 
\begin{equation}
\log p(\matY|\matX,\matW,\beta) = N_1 \log(1-\beta) + N_0\log\beta.
\end{equation}
It is straightforward to show that the MAP estimate for beta is
\begin{equation}
\beta = \frac{N_0}{N_0+N_1}.
\end{equation}


The optimisation subroutine is initialised as follows:

Cluster into $Q$ groups based on the \emph{Hamming distance} between two barcodes (the Hamming distance is equivalent to the \emph{edit distance}):
\begin{equation}
h(\vecy_i,\vecy_j) = \frac{1}{d}\sum_{\mu=1}^{d}\delta_{(1-y_{i\mu})y_{j\mu}}.
\end{equation}


The corrected read counts are inferred as follows:

For a given value of $Q$ we denote the value of the likelihood (\ref{eq:likelihood}) at the MAP parameter estimate by
\begin{equation}
L(Q) = p(\matY|\matX^*,\matW^*,\beta^*).
\end{equation}
The \emph{Bayes information criterion} (BIC) score is defined by
\begin{equation}
\text{BIC}(Q) = -2\log L(Q) + 2dQ\log N
\end{equation}
where $2dQ$ is the number of free parameters in the model. The \emph{corrected read count} is defined by
\begin{equation}
Q^* = \text{argmin}_Q \text{BIC}(Q).
\end{equation}


\subsubsection{Bayesian error correction script: The Code}

This analysis was performed in R:

\begin{lstlisting}
library(reshape2)

### Load and prepare a data file

# Length of barcode
D <- 7
# Load up one of the data files (needs to be in the current directory)
data <- read.csv("Samplename_length14_no_error_correction.csv",header=FALSE)
# vector of all the unique gRNA names
gRNA <- unique(data$V1)
# Total number of unique gRNAs
G <- length(gRNA)

### Generate datasets of barcodes

# Preallocate a list structure to hold the barcode datasets
Y <- vector('list',G)

# This loop goes through each gRNA, pulls out all the associated barcodes and puts them in a character matrix
for(mu in 1:G){
   ind <- which(data[[1]]==gRNA[mu])
   N <- length(ind)
   # Converts into character matrix (not the most elegant way...)
   Y[[mu]] <- matrix(as.vector(melt(lapply(as.character(data[[2]][ind]),strsplit,split=""))$value),nrow=N,ncol=2*D,byrow=TRUE)
}

### Fit model for each gRNA

# Preallocate a list of model resuls
res <- vector('list',G)

# Loop through gRNAs, for each one fit a model and get the corrected read count
# This can be parallelised for speed
for(mu in 1:G){
   # Begin tryCatch (catches any errors instead of stopping the loop)
   tryCatch({
      # Indices for barcodes matched that the current gRNA
      ind <- which(data[[1]]==gRNA[mu])
      # Vector of read counts
      counts <- data[[3]][ind]
      # Fit the model
      res[[mu]] <- fit_model(Y[[mu]], counts)
   }, error=function(e) NULL) #End tryCatch
} # End loop over gRNAs
\end{lstlisting}

This analysis calls on the following functions, named $fit_model$, $LL$ and $hamming$, to be defined:

\begin{lstlisting}
fit_model <- function(Y, counts){
   
   # Wrapper code for fitting a model to barcode count data. Returns a fitted model 
   # (see LL.R) and the corrected read count.
   #
   # Inputs:
   #    Y:       N by 2*D character matrix where each row is a barcode pair and 
   #             each element must equal T,C,G,A. D=7 is the barcode length.
   #    counts:  numeric vector of length N containing the read count for each barcode pair
   #    
   #
   # Outputs:
   #    model:  list where element q is the output from the LL function
   #    rho:    corrected read count
   #
   #
   # Copyright James Barrett 2015
   # Version: 1.0.0
   # Date: 9 Nov 2015
   # Contact: regmjeb@ucl.ac.uk
   
   
   # Total number of observed barcode pairs
   N <- length(counts)   
   # Preallocate a list for the output
   results <- list(model=vector('list',N), BIC=rep(NA,N))
   
   # ---------- Begin loop over N -------- #
   for (q in 1:N){
      
      # BREAK if there's only one barcode pair observed  
      if (N==1){
         results$rho <- 1
         break
      }
      
      # Fit model using LL function
      results$model[[q]] <- LL(Y, counts, q)
      results$BIC[q] <- results$model[[q]]$BIC
      results$rho <- which.min(results$BIC) # Current best estimate for corrected read count
      
      # BREAK if there are no mismatches and q=1. This means the model has achieved a 
      # perfect fit so we can stop.
      if ((results$model[[q]]$mismatches==0) & (q==1)){
         results$rho <- 1
         break
      }
      
      # BREAK if there are no mismatches. 
      if (results$model[[q]]$mismatches==0){
         results$rho <- which.min(results$BIC[1:(q-1)])
         break
      }
      
      # Search at least until q=10, then check to see if we've hit the minimum BIC score.
      # The BIC is non monotonic so don't search in the last five values of q.
      if ((q>=10) & (which.min(results$BIC[1:q])<(q-5))) break
   }
   # ---------- End loop over N -------- #
   
   return(results)
}


LL <- function(Y, counts, Q){
   
   # Fits a model to observed count data. Returns parameter estimates and BIC score.
   #
   # Inputs:
   #    Y:       N by 2*D character matrix where each row is a barcode pair and 
   #             each element must equal T,C,G,A. D=7 is the barcode length.
   #    counts:  numeric vector of length N containing the read count for each barcode pair
   #    Q:       scalar, number of latent components
   #    
   #
   # Outputs:
   #    W1:     numeric vector of length N where component i indicates which latent component
   #            the first barcode i was generated from. A value of zero means the second
   #            barcode i was associated with a latent barcode instead of the first.
   #    W2:     as above but for the second barcodes
   #    X1:     Q by D character matrix of latent first barcodes. Each row is a barcode. 
   #    X2:     as above but for the second barcodes
   #    matches: total number of matches between barcodes and associated observed barcodes 
   #    mismatches: total number of mismatches
   #    beta:   Noise hyperparameter
   #    BIC:    Bayes information criterion (used for model selection)
   #
   # Example:
   # Y <-  matrix(c("T","C","C","C","C","C","C","G","T","G","A","T","C","T",
   # "T","G","G","T","G","G","T","G","A","A","A","G","C","A",
   # "T","G","T","T","G","G","T","C","T","C","C","C","G","T"), 3, 14, byrow=T) 
   # counts <- c(1,3,1)
   # Q <- 1
   # result <- LL(Y, counts, Q)
   #
   #
   # Copyright James Barrett 2015
   # Version: 1.0.0
   # Date: 9 Nov 2015
   # Contact: regmjeb@ucl.ac.uk
   
   D <- 7         # Length of barcode
   N <- nrow(Y)   # Total number of pairs
   
   # Split matrix Y into two
   Y1 <- Y[,1:7]    # First 7bp barcodes
   Y2 <- Y[,8:14]   # Second barcodes
   
   
   #---------------------------------------------------------------#
   # Initial clustering (used to define initial guesses for the latent barcodes)
   #---------------------------------------------------------------#
   
   # Generate N by N matrix of pariwise Hamming distances for Y1
   H <- hamming(Y1)
   # Cluster hierarchically (use plot(fit) to visualise)
   fit <- hclust(as.dist(H), method = 'ward.D')
   # Use the clustering to split into Q clusters
   W1.new <- cutree(fit, k=Q)
   
   H <- hamming(Y2)
   fit <- hclust(as.dist(H), method = 'ward.D')
   W2.new <- cutree(fit, k=Q)
   
   # For speed (only marginal gain)
   count.matrix1 <- vector('list',D)
   count.matrix2 <- vector('list',D)
   r <- c("T","C","G","A")
   for(b in 1:D){
      count.matrix1[[b]] <- matrix(0,N,4)
      ind <- Y1[,b]=="T"
      count.matrix1[[b]][ind,1] <- counts[ind]
      ind <- Y1[,b]=="C"
      count.matrix1[[b]][ind,2] <- counts[ind]
      ind <- Y1[,b]=="G"
      count.matrix1[[b]][ind,3] <- counts[ind]
      ind <- Y1[,b]=="A"
      count.matrix1[[b]][ind,4] <- counts[ind]
      
      count.matrix2[[b]] <- matrix(0,N,4)
      ind <- Y2[,b]=="T"
      count.matrix2[[b]][ind,1] <- counts[ind]
      ind <- Y2[,b]=="C"
      count.matrix2[[b]][ind,2] <- counts[ind]
      ind <- Y2[,b]=="G"
      count.matrix2[[b]][ind,3] <- counts[ind]
      ind <- Y2[,b]=="A"
      count.matrix2[[b]][ind,4] <- counts[ind]
   }
   
   
   #---------------------------------------------------------------#
   # MAP barcode and W solutions
   #---------------------------------------------------------------#
   
   MAX <- 10      # Maximum number of iterations to find X and W
   counter <- 0   # Count how many iterations have been done so far
   
   # This while loop will find X, then W and interatively update their values until they
   # converge to stable values.
   
   while (counter < MAX){
      
      # Allocate Q by D matrix of latent barcodes for first barcode
      X1 <- matrix(0,Q,D)
      for (b in 1:Q){
         for(d in 1:D){
            # This line computes the most common nucleotide in all barcodes that were 
            # generated from latent barcode b (as specified by the value of W1. Read 
            # counts are taken into account.
            X1[b,d] <- r[which.max(colSums(count.matrix1[[d]][W1.new==b,,drop=FALSE]))]
         }
      }
      
      X2 <- matrix(0,Q,D)
      for (b in 1:Q){
         for(d in 1:D){
            X2[b,d] <- r[which.max(colSums(count.matrix2[[d]][W2.new==b,,drop=FALSE]))]
         }
      }
      
      # Next update the values of W
      W1.old <- W1.new   # Keep track of the old values so we can test if a stable solution has been reached
      W2.old <- W2.new
      for (i in 1:N){
         
         # H1 and H2 are vectors of edit distances between barcode i and the latent barcodes
         # Equivalent to Hamming distance in this case.
         H1 <- colSums(Y1[i,]!=t(X1))
         H2 <- colSums(Y2[i,]!=t(X2))
         
         # Test whether the first or second barcode has a smaller minimum edit distance for observation i
         if(min(H1)<=min(H2)){
            # If the first barcode is closer then W1.new[i] tells us which latent barcode i comes from
            W1.new[i] <- which.min(H1)   
            # The zero in W2.new[i] means i belongs to the one of the first barcodes
            W2.new[i] <- 0
         } else {
            W2.new[i] <- which.min(H2)   
            W1.new[i] <- 0
         }
      }
      
      # Stop if the solutions aren't changing anymore
      if (identical(W1.old,W1.new) & identical(W2.old,W2.new)) break
      counter <- counter + 1
   }
   W1 <- W1.new
   W2 <- W2.new
   
   
   #---------------------------------------------------------------#
   # MAP beta and BIC score
   #---------------------------------------------------------------#
   
   # Here we count the total number of nucleotide matches and mismatches
   
   N.match <- numeric(N)
   for (i in seq(1,N)){
      
      # Only cound matches between the observed and latent barcodes that have been associated with each other
      if(min(H1)<=min(H2)){
         N.match[i] <- sum(Y1[i,]==X1[W1[i],])   
      } else {
         N.match[i] <- sum(Y2[i,]==X2[W2[i],])
      }
   }
   
   matches <- sum(N.match*counts)
   mismatches <- D*sum(counts)-matches
   beta <- mismatches/(matches+mismatches) # Noise hyperparameter
   
   # Log likelihood
   LL <- matches*log(1-beta) + mismatches*log(beta)
   # Bayes information criterion score
   BIC <- -2*LL + (2*D)*Q*log(N)
   
   # Return a list with any relevant quantities
   return(list(W1=W1,W2=W2,X1=X1,X2=X2,matches=matches,mismatches=mismatches,beta=beta,BIC=BIC))
}

hamming <- function(Y){

   # Computes the pairwise Hamming distance between a matrix of genomic DNA sequences
   #
   # Inputs:
   #    Y:       N by D character matrix where each row is a barcode pair and 
   #             each element must equal T,C,G,A.
   #
   # Outpouts:
   #   H:       An N by N matrix of pairwise Hamming distances
   #
   #
   # Copyright James Barrett 2015
   # Version: 1.0.0
   # Date: 9 Nov 2015
   # Contact: regmjeb@ucl.ac.uk
   
   D <- 7
   N <- nrow(Y)
   
   YT <- Y=="T"
   HT <- YT %*% t(YT)
   
   YC <- Y=="C"
   HC <- YC %*% t(YC)
   
   YG <- Y=="G"
   HG <- YG %*% t(YG)
   
   YA <- Y=="A"
   HA <- YA %*% t(YA)
   
   H <- 1- (HT + HC + HG + HA)/D
   
   return(H)   
}

\end{lstlisting}


\subsection{Diagnostic plot: Number of UMI-corrected counts versus number of reads per gRNA}
\label{subsec:Diagnostic_plot Counts vs reads}

\subsubsection{Calculating the number of reads per gRNA}

To calculate the number of reads per gRNA for each sample, the following code was used:

\begin{lstlisting}
for i in *length14; do python ../Reads_per_gRNA.py "$i"; done
\end{lstlisting}



\begin{lstlisting}
# Reads_per_gRNA.py v 1.0
# Anna Koeferle Nov 2015, UCL
# Reads_per_gRNA.py counts the number of reads associated with each gRNA.
# This script takes the a tab-separated file as input:
# the Inputfile has the following columns: [0] read ID [1] gRNA chr:start-stop [2] 14 nt barcode
# the output is a csv file: [0] gRNA chr:start-stop , [1] number of reads

from __future__ import division
import collections as coll
import sys
import re
import numpy as np
import matplotlib.pyplot as plt
import pylab as pylab

####### Functions ######

def read_data(seqfilename):
    dict_seqfile = coll.defaultdict(list)   #make an empty collections default dictionary

    with open(seqfilename) as infile:
        for line in infile:
            lsplit = line.strip().split('\t')
            readID = lsplit[0]
            gRNA = lsplit[1]
            barcode = lsplit[2]
            dict_seqfile[gRNA].append(readID)
    return dict_seqfile


def write_dict_to_csv(dict_name, outfilename):
    with open(outfilename, 'w') as outfile:
        for key in dict_name:
            outfile.write(str(key) + ', ' + str(len(dict_name[key])) + '\n')


def sanity_check(dict_seqfile):
    sanity = {}  ### calculate total number of reads in sample and crosscheck
    for key in dict_seqfile:
        sanity[key] = len(dict_seqfile[key])

    sumcheck = 0
    for key in sanity:
        sumcheck += sanity[key]

    return sumcheck

######Script#####

if (len(sys.argv) <> 2):
    print "Missing inputfile! Usage: python Reads_per_gRNA.py INPUTFILENAME"
    sys.exit()
else:
    seqfilename = str(sys.argv[1])
    outfilename = seqfilename.split(".")[0]+str("_read_count")

    print "\nReading", str(seqfilename), "into dictionary...and printing to csv..."

    dict_seqfile = read_data(seqfilename)

    write_dict_to_csv(dict_seqfile, outfilename)

    total_number_of_reads = sanity_check(dict_seqfile)

    print "Done! The total number of reads for this sample were: " + str(total_number_of_reads)

\end{lstlisting}


\subsubsection{Wrapping gRNA counts of all samples into a table}

The output of the collapse-barcodes.py script was formatted into a table as follows (The Bayesian model outputs a csv file of counts per gRNA for each sample):

\begin{lstlisting}
# MakeTable_from_counts.py v 1.0
# Anna Koeferle OCt 2015, UCL
# This script takes the output of collapse_barcodes.py (either raw or no_orphans table) and formats the output into a dataframe for use with PCA and mageck scripts

import numpy as np
import pandas as pd
import fileinput
import sys
from time import time, clock
import pybedtools
from pybedtools import BedTool

####Global variables#####

### load library file into pybedtools BedTool object
### if using a library other than EMT5000 library, need to include path to library file here. Expects tab separated bed file of genomic target regions with associated target gene name
### e.g. chr "\t" start "\t" stop "\t" target gene name

EMT_lib = pybedtools.BedTool('/Users/anna_koeferle/Documents/UCL/HiSeqRun_Sept2015/gRNA_counts/EMT5000_library_regions.bed')

####Functions####

def load_file_to_dict(samplename):
    my_dict = {}
    with open(samplename, 'r') as infile:
        name = samplename
        for line in infile:
            lsplit = line.strip().split(', ')
            gRNA = lsplit[0]
            count = lsplit[1]
            my_dict[gRNA] = count
    return (my_dict, name) # tuple

# load_file_to_dict takes a filename as input and loads the corresponding file into a dictionary with keys: gRNA and values : gRNA count
# the inputfile is a comma-separated file of the form gRNA (chr:start-stop), count. This file is output by collapse_barcodes.py

def make_data_frame_from_dict(tuple_dict_name): #takes tuple
    my_dict = tuple_dict_name[0]
    samplename = tuple_dict_name[1]
    my_df = pd.DataFrame.from_dict(my_dict, orient='index')
    my_df.columns = [samplename.split("/")[-1].split("_")[0]]
    return my_df

# make_data_frame_from_dict takes a dictionary as input and makes it into a pandas dataframe using the keys as row indices

def Find_target_gene(my_dataframe, gRNA_library_with_genes):
    ###get index out of dataframe and into bed format
    gRNA_list =list(my_dataframe.index.values)

    gRNA_list_formatted = []

    for gRNA in gRNA_list:
        lsplit = gRNA.split (':')
        chrom = lsplit[0]
        startstop = lsplit[1].split('-')
        start = startstop[0]
        stop = startstop[1]
        gRNA_list_formatted.append( chrom + " " + start + " " + stop + " \n")


    gRNA_string = ' '.join(gRNA_list_formatted)
    gRNA_bed = BedTool(gRNA_string, from_string=True)
    gRNA_with_gene = gRNA_bed.intersect(EMT_lib, f=1, wa=True, wb=True)

    target_gene_name = [(f[6]) for f in gRNA_with_gene]
    my_dataframe.insert(0, "gene", target_gene_name)
    return my_dataframe


def catenate_dataframes():
    result = pd.DataFrame()  # make an empty pandas data frame

    for sample in sys.argv[1:]:   # loop over each file in the list supplied by the user
        new_df = make_data_frame_from_dict(load_file_to_dict(sample))  # get the dataframe returned when running the functions load_file_to_dict and make_data_frame_from_dict
        result = pd.concat([result, new_df], axis=1)  # update the empty data frame with the additional sample as a separate column

    result = Find_target_gene(result, EMT_lib) # inserts a column of target gene names for the gRNAs in the index column
    result.index.name = 'sgRNA'
    return result

###Script####

if (len(sys.argv) < 2):
    print "Missing inputfiles! Usage: python MakeTable_from_counts.py INPUTFILE1 INPUTFILE2 ... INPUTFILEn"
    sys.exit()

else:
    print "\nReading inputfile(s) into dictionary..."
    t0 = time() # Begin timer

    dataframe_output = catenate_dataframes()

    print "\nGeneratig outputfile..."
    dataframe_output.to_csv('Dataframe.txt', sep='\t', na_rep= '0') # na_rep tells what to output instead of NAs need to check what mageck can deal with

    timed = time() - t0
    print '\t Finished! Took', round(timed,3), 'seconds'

\end{lstlisting}

\subsubsection{Plotting counts versus number of reads}

The number of reads per gRNA were plotted against the counts per gRNA, derived either without error correction, with naive PCR error correction or Bayesian PCR error correction as described above, using the following code:

\begin{lstlisting}
#plot_counts_vs_number_of_reads.py v 1.0.0
# Anna Koeferle, UCL, November 2015
#takes as input a dataframe holding number of reads per gRNA, a dataframe holding number of counts per gRNA and a samplesheet, listing the samples to be processed.

#####USAGE######

# python plot_counts_vs_number_of_reads.py [Dataframe-Counts.csv] [Dataframe-NumberOfReads.csv] [Path/to/list_of_Samplenames]


from __future__ import division
import numpy as np
import pandas as pd
import sys
import re
import matplotlib.pyplot as plt
import pylab as pylab


####Functions####

def find_delimiter(filename, delimiters):
    #checks if file is csv or tsv and loads file into pandas dataframe
    for delim in delimiters:
        df_counts = pd.DataFrame.from_csv(filename, header=0, sep= delim, index_col=0)
        if len(df_counts.columns) > 0:
            print 'Delimiter found!'
            break
        else:
            print 'Have not found the correct delimiter yet. Still searching....'
            df_counts = None
    return df_counts

def remove_underscore_column_names(dataset):
    dataset_renamed = dataset.rename(columns=lambda x: re.sub(r"_.*", "",x))
    return dataset_renamed

def remove_duplicates_from_df(dfname):
    new_df = dfname.T.groupby(level=0).first().T
    return new_df

def plot_scatterplot(sample):
    new_df = pd.concat([df_reads[sample], df_counts[sample]], axis =1, keys=['reads', 'counts'])
    fig = plt.figure()
    plt.scatter(new_df['reads'], new_df['counts'], c='steelblue')
    plt.xlabel('number of reads', fontsize=16)
    plt.ylabel('number of counts (unique molecules)', fontsize=16)
    fig.suptitle(str(sample), fontsize=24)
    plt.savefig(str(sample) + '.png', format='png', dpi=300)
    plt.close()

#####Checking user input#####

if (len(sys.argv) <> 4):
    print "Missing inputfile(s)! Usage: python plot_counts_vs_number_of_reads.py [Dataframe-Counts] [Dataframe-NumberOfReads] [Samplesheet]"
    sys.exit()

else:
    dfname_counts = str(sys.argv[1])
    dfname_reads = str(sys.argv[2])
    name_samplefile = str(sys.argv[3])

    delimiters = ['\t',', ', ',']

    ####load the count dataframe:
    df_counts = find_delimiter(dfname_counts, delimiters)
    if str(type(df_counts)) == "<type 'NoneType'>":
        print 'Delimiter not found in' + str(dfname_counts) + 'Please try again. Acceptable inputdataframes are tab-separated or comma-separated files.'
        sys.exit()

    df_counts = remove_duplicates_from_df(remove_underscore_column_names(df_counts))

    ###load the read dataframe:
    df_reads = find_delimiter(dfname_reads, delimiters)
    if str(type(df_reads)) == "<type 'NoneType'>":
        print 'Delimiter not found in' + str(dfname_reads) + 'Please try again. Acceptable inputdataframes are tab-separated or comma-separated files.'
        sys.exit()

    df_reads = remove_duplicates_from_df(remove_underscore_column_names(df_reads))

    ####load the samplesheet into a list:
    samplefile =[]
    with open(name_samplefile) as infile:
        for line in infile: #remove everything after the first underscoe and remove newline
            samplefile.append(re.sub(r"_.*", "", str(line.rstrip("\n"))))

    for sample in samplefile:
        try:
            plot_scatterplot(sample)
        except Exception, msg:
            print str(msg)

\end{lstlisting}

\subsection{Diagnostic plot: compare gRNA counts before and after PCR error correction}

The below python script takes two dataframes of gRNA counts (e.g. with vs without PCR error correction) and plots one against the other.

\begin{lstlisting}
#plot_counts_vs_counts.py v 1.0.0
# Anna Koeferle, UCL, November 2015
#takes as input two dataframes (column: samplename, rows: gRNA counts, index: gRNA as chr:start-stop) holding number of counts per gRNA, and a samplesheet, listing the samples to be processed


from __future__ import division
import numpy as np
import pandas as pd
import sys
import re
import matplotlib.pyplot as plt
import pylab as pylab

#####USAGE######

# python plot_counts_vs_counts_totalcountnorm.py [Dataframe-Counts-xaxis.csv] [Dataframe-Counts-yaxis.csv] [Path/to/list_of_Samplenames]

####Functions####

def find_delimiter(filename, delimiters):
    #checks if file is csv or tsv and loads file into pandas dataframe
    for delim in delimiters:
        df_counts = pd.DataFrame.from_csv(filename, header=0, sep= delim, index_col=0) #read in file using first delimiter in list of possible deliminters
        if len(df_counts.columns) > 0: #check if using this delimiter has split the file correctly into columns
            print 'Delimiter found!'
            break
        else:
            print 'Have not found the correct delimiter yet. Still searching....'
            df_counts = None #set the delimiter to None if it was incorrect
    return df_counts

def remove_underscore_column_names(dataset):
    dataset_renamed = dataset.rename(columns=lambda x: re.sub(r"_.*", "",x)) #removes everything in sample name after first occurence of an underscore
    return dataset_renamed

def remove_duplicates_from_df(dfname):
    new_df = dfname.T.groupby(level=0).first().T #removes duplicate colunns from df
    return new_df

def plot_scatterplot(sample):
    new_df = pd.concat([df_counts1[sample], df_counts2[sample]], axis =1, keys=['counts1', 'counts2'])
    fig,ax = plt.subplots() # use this to get ax object
    ax.scatter(new_df['counts1'], new_df['counts2'], c='midnightblue', label = None) # plots the scatterplot
    if dfname_counts1.split('/')[-1] == 'bayesian_corrected.csv':
        ax.set_xlabel('counts (Bayesian error correction)', fontsize=16) # label for x axis
    elif dfname_counts1.split('/')[-1] == 'Dataframe_allsamples.txt':
        ax.set_xlabel('counts (PCR error correction max 4mm)', fontsize=16) # label for x axis
    elif dfname_counts1.split('/')[-1] == 'Dataframe_allsamples_no_errors.txt':
        ax.set_xlabel('counts (no error correction)', fontsize=16) # label for x axis
    else:
        ax.set_xlabel('Unknown', fontsize=16) # label for x axis

    if dfname_counts2.split('/')[-1] == 'bayesian_corrected.csv':
        ax.set_ylabel('counts (Bayesian error correction)', fontsize=16) # label for y axis
    elif dfname_counts2.split('/')[-1] == 'Dataframe_allsamples.txt':
        ax.set_ylabel('counts (PCR error correction max 4mm)', fontsize=16) # label for y axis
    elif dfname_counts2.split('/')[-1] == 'Dataframe_allsamples_no_errors.txt':
        ax.set_ylabel('counts (no error correction)', fontsize=16) # label for y axis
    else:
        ax.set_ylabel('Unknown', fontsize=16) # label for y axis

    fig.suptitle(str(sample), fontsize=24) #figure title
    xdim = ax.get_xlim() # from the x object get the range of the data, returns a tuple
    xmin = xdim[0] # subset the tuple
    xmax = xdim[1]
    ydim = ax.get_ylim()
    ymin = ydim[0]
    ymax = ydim[1]
    identity_line = np.linspace(max(xmin, ymin),min(xmax, ymax)) #derive a  numpy array and get min and max comparing the values
    ax.plot(identity_line, identity_line, color="darkorange", linewidth=2.0, label='x = y') # plot the identity line x=y
    ax.axis([xmin,xmax,ymin,ymax]) #rescale the axes so the identity line goes to edge of box
    ax.legend(loc =4, frameon =False)
    plt.savefig(str(sample) + '.png', dpi=300)
    plt.close()

#####Checking user input#####

if (len(sys.argv) <> 4):
    print "Missing inputfile(s)! Usage: python plot_counts_vs_counts_totalcountnorm.py [Dataframe1-Counts] [Dataframe2-Counts] [Samplesheet]"
    sys.exit()

else:
    dfname_counts1 = str(sys.argv[1])
    dfname_counts2 = str(sys.argv[2])
    name_samplefile = str(sys.argv[3])

    delimiters = ['\t',', ', ',']

    ####load the count dataframe:
    df_counts1 = find_delimiter(dfname_counts1, delimiters)
    if str(type(df_counts1)) == "<type 'NoneType'>":
        print 'Delimiter not found in' + str(dfname_counts1) + 'Please try again. Acceptable inputdataframes are tab-separated or comma-separated files.'
        sys.exit()

    df_counts1 = remove_duplicates_from_df(remove_underscore_column_names(df_counts1))

    ###load the read dataframe:
    df_counts2 = find_delimiter(dfname_counts2, delimiters)
    if str(type(df_counts2)) == "<type 'NoneType'>":
        print 'Delimiter not found in' + str(dfname_counts2) + 'Please try again. Acceptable inputdataframes are tab-separated or comma-separated files.'
        sys.exit()

    df_counts2 = remove_duplicates_from_df(remove_underscore_column_names(df_counts2))

    ####load the samplesheet into a list:
    samplefile =[]
    with open(name_samplefile) as infile:
        for line in infile: #remove everything after the first underscoe and remove newline
            samplefile.append(re.sub(r"_.*", "", str(line.rstrip("\n"))))

    for sample in samplefile:
        try:
            plot_scatterplot(sample)
        except Exception, msg:
            print str(msg)

\end{lstlisting}

\subsection{Diagnostic plot: Counts versus number of sorted cells}

This script accepts three user-arguments, a dataframe of gRNA counts (c), a dataframe of numbers of sorted cells per sample (n), and a samplesheet (s) and returns a scatterplot of sorted cells versus counts with one data point per sample in the samplesheet.

\begin{lstlisting}
### Cellnumber_vs_counts.py v 1.0
### Anna Koeferle Nov 2015

###USAGE###
# python Cellnumber_vs_counts.py

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import matplotlib.colors as mplcolors
import matplotlib as mpl
import argparse
import sys

###Functions####

def find_delimiter(filename, delimiters):
    #checks if file is csv or tsv and loads file into pandas dataframe
    for delim in delimiters:
        df_counts = pd.DataFrame.from_csv(filename, header=0, sep= delim, index_col=0)
        if len(df_counts.columns) > 0:
            print 'Delimiter found!'
            break
        else:
            print 'Have not found the correct delimiter yet. Still searching....'
            df_counts = None
    return df_counts

def remove_underscore_column_names(dataset):
    dataset_renamed = dataset.rename(columns=lambda x: re.sub(r"_.*", "",x))
    return dataset_renamed

def remove_duplicates_from_df(dfname):
    new_df = dfname.T.groupby(level=0).first().T
    return new_df

def replace_underscore_with_dash(dataset):
    dataset_renamed = dataset.rename(index=lambda x: re.sub(r"_", "-",x))
    return dataset_renamed

def guess_error_correction_method():
    if args.counts.split('/')[-1] == 'bayesian_corrected.csv' or args.counts.split('/')[-1] == 'bayesian_corrected_new.csv':
        method = 'Bayesian error correction'
    elif args.counts.split('/')[-1] == 'Dataframe_allsamples.txt':
        method = 'Barcodes grouped if < 4MM'
    elif args.counts.split('/')[-1] == 'Dataframe_allsamples_no_errors.txt':
        method = 'no error correction'
    else:
        method = 'unkown'
    return method

def restrict_to_samplesheet(mydataframe):
    samplefile = []
    new_df = pd.DataFrame()
    with open(args.samplesheet) as infile:
        for line in infile: #remove everything after the first underscroe and remove newline
            samplefile.append(re.sub(r"_.*", "", str(line.rstrip("\n"))))

    for sample in samplefile:
        if sample in mydataframe.index:
            print str(sample) + ' is in the dataset.'
            new_df = new_df.append(pd.Series(mydataframe.loc[sample, :]))  #append this index and all columns to dataframe
        else:
            print str(sample) + ' is NOT in the dataset.'

    return new_df


def plot_scatter(x_count, y_cellNumber, colorvar):
    fig,ax = plt.subplots() # use this to get ax object
    colors = ['midnightblue', 'darkorange']
    levels = [2, 3]
    cmap, norm = mplcolors.from_levels_and_colors(levels=levels, colors=colors, extend='max')

    myscatterplot = ax.scatter(x_count, y_cellNumber, c=colorvar, cmap=cmap, norm = norm, edgecolors='none', s=40) # plots the scatterplot

    colormap=myscatterplot.get_cmap()  #get the colormap used for the plot
    #use this to make proxy artists for the legend
    circles_artist=[mpl.lines.Line2D(range(1), range(1), color='w', marker='o', markersize=6, markeredgewidth=0, markerfacecolor=item) for item in colormap((np.array([2,3])-2)/1)]

    # mpl.lines.Line2D draws the circular object
    # using the colors from the list comprehension
    # colormap((np.array([2,3])-2)/1) # gets the colors out the colormap used for the plot. the array bit normalises the values to a range between 0-1 used for mapping

    ax.set_xlabel('Sum of gRNA counts', fontsize=16) # label for x axis
    ax.set_ylabel('Number of sorted cells', fontsize=16) # label for y axis

    method = guess_error_correction_method()
    title = fig.suptitle('All libraries - ' + str(method), fontsize=22) #figure title

    xdim = ax.get_xlim() # from the x object get the range of the data, returns a tuple
    xmin = xdim[0] # subset the tuple
    xmax = xdim[1]
    ydim = ax.get_ylim()
    ymin = ydim[0]
    ymax = ydim[1]
    identity_line = np.linspace(max(xmin, ymin),min(xmax, ymax)) #derive a  numpy array and get min and max comparing the values
    identity_plot = ax.plot(identity_line, identity_line, color="dimgrey", linewidth=2.0, label='x = y') # plot the identity line x=y

    line_artist = mpl.lines.Line2D([],[], color='dimgrey', linewidth=2.0, label='x = y') # draws the grey line in the legend
    ax.axis([xmin,xmax,ymin,ymax]) #rescale the axes so the identity line goes to edge of box

    line_legend = ax.legend(handles =[line_artist], loc = 4, frameon=False) #first legend is drawn inside the plot reads x= y
    # Add the legend for the line manually to the current Axes. This is the trick allowing us to plot two different legends!
    ax = plt.gca().add_artist(line_legend)
    # Create another legend for coloured points
    #lt.legend(handles=[line2], loc=4)

    circle_legend = plt.legend(circles_artist, ['2 cycles', '3 cycles'], loc = "center left", bbox_to_anchor = (1, 0.5), frameon =False, numpoints =1)
    plt.savefig('Sorted_cells_vs_gRNA_counts_' + str(method) + '.png', bbox_extra_artists=(circle_legend, title), bbox_inches='tight', format='png', dpi=300) ## bbox extra artist and bbox inches makes sure legend is not cut off figure
    plt.close()


###User input###

parser = argparse.ArgumentParser()
parser.add_argument("-c", "--counts", help="dataframe containing counts per gRNA")
parser.add_argument("-n", "--cellnumber", help="dataframe containing counts of sorted cells")
parser.add_argument("-s", "--samplesheet", help="samplesheet containing one sample name per line, exactly as in dataframes")
args = parser.parse_args()

if not args.counts or not args.cellnumber:
   sys.exit("Missing inputfile!")

print "Reading data...."

delimiters = ['\t',', ', ',']

df_counts = find_delimiter(args.counts, delimiters)
if str(type(df_counts)) == "<type 'NoneType'>":
    print 'Delimiter not found in' + str(dfname_counts) + 'Please try again. Acceptable inputdataframes are tab-separated or comma-separated files.'
    sys.exit()

df_counts = remove_duplicates_from_df(remove_underscore_column_names(df_counts))
df_total_counts = pd.DataFrame(df_counts.sum(axis=0), columns = ['TotalCount'])
df_total_counts.index.name = None

number_of_cells = find_delimiter(args.cellnumber, delimiters)
if str(type(df_counts)) == "<type 'NoneType'>":
    print 'Delimiter not found in' + str(dfname_counts) + 'Please try again. Acceptable inputdataframes are tab-separated or comma-separated files.'
    sys.exit()

cell_count = remove_duplicates_from_df(remove_underscore_column_names(number_of_cells))
cell_count.drop(cell_count.columns[2:], axis=1, inplace=True)
cell_count = replace_underscore_with_dash(cell_count)
cell_count.index.name = None

frames = [df_total_counts, cell_count]
df_counts_cellnumber = pd.concat(frames, axis =1)
df_counts_cellnumber = df_counts_cellnumber.dropna(axis = 'index', how = 'any') #remove any rows that contain an NA
df_counts_cellnumber = restrict_to_samplesheet(df_counts_cellnumber) #makes sure only samples that are in the samplesheet are plotted in next step

plot_scatter(df_counts_cellnumber['TotalCount'], df_counts_cellnumber['CellCount'], df_counts_cellnumber['Cycles'])

print "Done!"
\end{lstlisting}


\subsection{Enrichment analysis: Naive approach}


\subsection{Enrichment analysis using DESeq2}

\subsubsection{Preparing Bayesian error correction output for DESeq2}
DeSeq requires for each experiment a list of counts per gRNA.
The data for each experiment were extracted from the Bayesian analysis output. gRNAs where 3/4 of the counts in a given experiment are 0 (or NA) are removed before enrichment analysis as follows:

\begin{lstlisting}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import collections as coll
import re
import pylab

###Functions###

def remove_underscore_column_names(dataset):
    dataset_renamed = dataset.rename(columns=lambda x: re.sub(r"_.*", "",x))
    return dataset_renamed
    
def generate_samplefile(samplefilename):
    samplefile =[]
    with open(samplefilename) as infile:
        for line in infile: #remove everything after the first underscoe and remove newline
            samplefile.append(re.sub(r"_.*", "", str(line.rstrip("\n"))))
    return samplefile
    
def remove_3quarters_NAs(dataframe):
    df_with_NAs = dataframe.replace('0', np.nan)
    thresh = int(len(df_with_NAs.columns)/4*3)
    NA_removed =df_with_NAs.dropna(axis='index', thresh=thresh)
    return NA_removed
    
 def save_df_for_samplefile(samplefile, samplefilename): #this function calls all other functions
    df_samplefile = df_bayes[samplefile]
    df_samplefile = remove_3quarters_NAs(df_samplefile)
    df_samplefile = df_samplefile.fillna(0)
    df_samplefile.to_csv(str(samplefilename)+'_counts.csv')

###Script###

df_bayes = pd.DataFrame.from_csv('path_2/bayesian_corrected_counts.csv', header=0, sep=',', index_col=0)

samplefilename = 'path_2/samplefile.txt'
samplefile =generate_samplefile(samplefilename)

save_df_for_samplefile(samplefile, samplefilename)
\end{lstlisting}

\subsection{Correlation of Log2Fold enrichment scores from DESeq2 between experiments}

\begin{lstlisting}
library ("DESeq2")

experiment_info <- read.table("path2/ThisExperiment_DeSeq2_ColData.csv", sep=",", header=TRUE, blank.lines.skip = TRUE, row.names = "Sample.name")
counts<-read.table("path2/ThisExperiment_counts_bayes_new.csv", sep=",", header=TRUE, na.strings="NaN", check.names=FALSE, row.names=1)


counts_Batch5[is.na(counts_Batch5)] <- 0  #replace NAs with 0
dds_Batch5<- DESeqDataSetFromMatrix(countData=counts_Batch5, colData = experiment_info_Batch5, design = ~treatment)
#converting counts to integer mode
dds_Batch5<-DESeq(dds_Batch5)

res<-results(dds_Batch5)
resOrdered <- res[order(res$padj),]
resOrdered
\end{lstlisting}



\subsubsection{Visualising enrichment}




\end{footnotesize}
